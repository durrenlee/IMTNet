MultiScaleLgtFormer(
  (patch_embed): PatchEmbed(
    (proj): Sequential(
      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): GELU(approximate='none')
      (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): GELU(approximate='none')
      (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): GELU(approximate='none')
      (9): Conv2d(64, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    )
  )
  (stages): ModuleList(
    (0): LocalStage(
      (blocks): ModuleList(
        (0): LocalBlock(
          (pos_embed): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)
          (norm1): LayerNorm((72,), eps=1e-06, elementwise_affine=True)
          (attn): MultiDilateLocalAttention(
            (qkv): Conv2d(72, 216, kernel_size=(1, 1), stride=(1, 1))
            (dilate_attention): ModuleList(
              (0): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (1): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=2, padding=2, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (2): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=3, padding=3, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (dwconv_attn): DWConvAttention(
              (dwconv): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)
            )
            (proj): Linear(in_features=72, out_features=72, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((72,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=72, out_features=288, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=288, out_features=72, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): LocalBlock(
          (pos_embed): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)
          (norm1): LayerNorm((72,), eps=1e-06, elementwise_affine=True)
          (attn): MultiDilateLocalAttention(
            (qkv): Conv2d(72, 216, kernel_size=(1, 1), stride=(1, 1))
            (dilate_attention): ModuleList(
              (0): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (1): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=2, padding=2, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (2): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=3, padding=3, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (dwconv_attn): DWConvAttention(
              (dwconv): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)
            )
            (proj): Linear(in_features=72, out_features=72, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.008)
          (norm2): LayerNorm((72,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=72, out_features=288, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=288, out_features=72, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        (proj): Sequential(
          (0): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (1): LocalStage(
      (blocks): ModuleList(
        (0): LocalBlock(
          (pos_embed): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)
          (attn): MultiDilateLocalAttention(
            (qkv): Conv2d(144, 432, kernel_size=(1, 1), stride=(1, 1))
            (dilate_attention): ModuleList(
              (0): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (1): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=2, padding=2, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (2): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=3, padding=3, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (dwconv_attn): DWConvAttention(
              (dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
            )
            (proj): Linear(in_features=144, out_features=144, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.015)
          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=144, out_features=576, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=576, out_features=144, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): LocalBlock(
          (pos_embed): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)
          (attn): MultiDilateLocalAttention(
            (qkv): Conv2d(144, 432, kernel_size=(1, 1), stride=(1, 1))
            (dilate_attention): ModuleList(
              (0): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (1): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=2, padding=2, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (2): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=3, padding=3, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (dwconv_attn): DWConvAttention(
              (dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
            )
            (proj): Linear(in_features=144, out_features=144, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.023)
          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=144, out_features=576, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=576, out_features=144, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): LocalBlock(
          (pos_embed): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)
          (attn): MultiDilateLocalAttention(
            (qkv): Conv2d(144, 432, kernel_size=(1, 1), stride=(1, 1))
            (dilate_attention): ModuleList(
              (0): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (1): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=2, padding=2, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (2): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=3, padding=3, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (dwconv_attn): DWConvAttention(
              (dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
            )
            (proj): Linear(in_features=144, out_features=144, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.031)
          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=144, out_features=576, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=576, out_features=144, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): LocalBlock(
          (pos_embed): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)
          (attn): MultiDilateLocalAttention(
            (qkv): Conv2d(144, 432, kernel_size=(1, 1), stride=(1, 1))
            (dilate_attention): ModuleList(
              (0): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (1): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=2, padding=2, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
              (2): DilateAttention(
                (unfold): Unfold(kernel_size=3, dilation=3, padding=3, stride=1)
                (attn_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (dwconv_attn): DWConvAttention(
              (dwconv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
            )
            (proj): Linear(in_features=144, out_features=144, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.038)
          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=144, out_features=576, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=576, out_features=144, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        (proj): Sequential(
          (0): Conv2d(144, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (2): DCNStage(
      (blocks): ModuleList(
        (0): DCNBlock(
          (pos_embed): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (dcn): DCNv4(
            (offset_mask_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
            (offset_mask): Linear(in_features=288, out_features=488, bias=True)
            (value_proj): Linear(in_features=288, out_features=288, bias=True)
            (output_proj): Linear(in_features=288, out_features=288, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.046)
          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=288, out_features=1152, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1152, out_features=288, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): DCNBlock(
          (pos_embed): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (dcn): DCNv4(
            (offset_mask_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
            (offset_mask): Linear(in_features=288, out_features=488, bias=True)
            (value_proj): Linear(in_features=288, out_features=288, bias=True)
            (output_proj): Linear(in_features=288, out_features=288, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.054)
          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=288, out_features=1152, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1152, out_features=288, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): DCNBlock(
          (pos_embed): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (dcn): DCNv4(
            (offset_mask_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
            (offset_mask): Linear(in_features=288, out_features=488, bias=True)
            (value_proj): Linear(in_features=288, out_features=288, bias=True)
            (output_proj): Linear(in_features=288, out_features=288, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.062)
          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=288, out_features=1152, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1152, out_features=288, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): DCNBlock(
          (pos_embed): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (dcn): DCNv4(
            (offset_mask_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
            (offset_mask): Linear(in_features=288, out_features=488, bias=True)
            (value_proj): Linear(in_features=288, out_features=288, bias=True)
            (output_proj): Linear(in_features=288, out_features=288, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.069)
          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=288, out_features=1152, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1152, out_features=288, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): DCNBlock(
          (pos_embed): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (dcn): DCNv4(
            (offset_mask_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
            (offset_mask): Linear(in_features=288, out_features=488, bias=True)
            (value_proj): Linear(in_features=288, out_features=288, bias=True)
            (output_proj): Linear(in_features=288, out_features=288, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.077)
          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=288, out_features=1152, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1152, out_features=288, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): DCNBlock(
          (pos_embed): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (dcn): DCNv4(
            (offset_mask_dw): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)
            (offset_mask): Linear(in_features=288, out_features=488, bias=True)
            (value_proj): Linear(in_features=288, out_features=288, bias=True)
            (output_proj): Linear(in_features=288, out_features=288, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.085)
          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=288, out_features=1152, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1152, out_features=288, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        (proj): Sequential(
          (0): Conv2d(288, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (3): DCNStage(
      (blocks): ModuleList(
        (0): DCNBlock(
          (pos_embed): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)
          (dcn): DCNv4(
            (offset_mask_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
            (offset_mask): Linear(in_features=576, out_features=976, bias=True)
            (value_proj): Linear(in_features=576, out_features=576, bias=True)
            (output_proj): Linear(in_features=576, out_features=576, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.092)
          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=576, out_features=2304, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2304, out_features=576, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): DCNBlock(
          (pos_embed): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)
          (dcn): DCNv4(
            (offset_mask_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
            (offset_mask): Linear(in_features=576, out_features=976, bias=True)
            (value_proj): Linear(in_features=576, out_features=576, bias=True)
            (output_proj): Linear(in_features=576, out_features=576, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=576, out_features=2304, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=2304, out_features=576, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        (proj): Sequential(
          (0): Conv2d(576, 1152, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
  )
  (norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1152, out_features=100, bias=True)
)
| module                       | #parameters or shape   | #flops     |
|:-----------------------------|:-----------------------|:-----------|
| model                        | 22.938M                | 3.496G     |
|  patch_embed.proj            |  65.168K               |  0.445G    |
|   patch_embed.proj.0         |   0.432K               |   21.676M  |
|    patch_embed.proj.0.weight |    (16, 3, 3, 3)       |            |
|   patch_embed.proj.1         |   32                   |   1.606M   |
|    patch_embed.proj.1.weight |    (16,)               |            |
|    patch_embed.proj.1.bias   |    (16,)               |            |
|   patch_embed.proj.3         |   4.608K               |   57.803M  |
|    patch_embed.proj.3.weight |    (32, 16, 3, 3)      |            |
|   patch_embed.proj.4         |   64                   |   0.803M   |
|    patch_embed.proj.4.weight |    (32,)               |            |
|    patch_embed.proj.4.bias   |    (32,)               |            |
|   patch_embed.proj.6         |   18.432K              |   0.231G   |
|    patch_embed.proj.6.weight |    (64, 32, 3, 3)      |            |
|   patch_embed.proj.7         |   0.128K               |   1.606M   |
|    patch_embed.proj.7.weight |    (64,)               |            |
|    patch_embed.proj.7.bias   |    (64,)               |            |
|   patch_embed.proj.9         |   41.472K              |   0.13G    |
|    patch_embed.proj.9.weight |    (72, 64, 3, 3)      |            |
|  stages                      |  22.755M               |  3.051G    |
|   stages.0                   |   0.223M               |   0.48G    |
|    stages.0.blocks           |    0.129M              |    0.407G  |
|    stages.0.downsample.proj  |    93.744K             |    73.382M |
|   stages.1                   |   1.388M               |   0.87G    |
|    stages.1.blocks           |    1.014M              |    0.797G  |
|    stages.1.downsample.proj  |    0.374M              |    73.27M  |
|   stages.2                   |   7.371M               |   1.223G   |
|    stages.2.blocks           |    5.876M              |    1.15G   |
|    stages.2.downsample.proj  |    1.495M              |    73.213M |
|   stages.3                   |   13.773M              |   0.477G   |
|    stages.3.blocks           |    7.798M              |    0.382G  |
|    stages.3.downsample.proj  |    5.975M              |    95.588M |
|  norm                        |  2.304K                |  92.16K    |
|   norm.weight                |   (1152,)              |            |
|   norm.bias                  |   (1152,)              |            |
|  head                        |  0.115M                |  0.115M    |
|   head.weight                |   (100, 1152)          |            |
|   head.bias                  |   (100,)               |            |
creating model_ema...
number of params: 22938212
criterion:SoftTargetCrossEntropy()
args.distillation_type:none
criterion wrapped by DistillationLoss:DistillationLoss(
  (base_criterion): SoftTargetCrossEntropy()
)
loading checkpoint dict from file:/root/output/cifar/checkpoint.pth
loading model from checkpoint dict.
Test:  [  0/348]  eta: 0:07:14  loss: 0.2453 (0.2453)  acc1: 100.0000 (100.0000)  acc5: 100.0000 (100.0000)  time: 1.2499  data: 0.5282  max mem: 1505
Test:  [ 10/348]  eta: 0:01:18  loss: 0.2459 (0.2437)  acc1: 100.0000 (99.8106)  acc5: 100.0000 (100.0000)  time: 0.2311  data: 0.0588  max mem: 1506
Test:  [ 20/348]  eta: 0:00:59  loss: 0.2461 (0.2489)  acc1: 100.0000 (99.6693)  acc5: 100.0000 (100.0000)  time: 0.1293  data: 0.0118  max mem: 1506
Test:  [ 30/348]  eta: 0:00:52  loss: 0.2447 (0.2483)  acc1: 99.3056 (99.6640)  acc5: 100.0000 (100.0000)  time: 0.1296  data: 0.0119  max mem: 1506
Test:  [ 40/348]  eta: 0:00:48  loss: 0.2447 (0.2483)  acc1: 100.0000 (99.6951)  acc5: 100.0000 (100.0000)  time: 0.1295  data: 0.0118  max mem: 1506
Test:  [ 50/348]  eta: 0:00:45  loss: 0.2463 (0.2484)  acc1: 100.0000 (99.7141)  acc5: 100.0000 (100.0000)  time: 0.1296  data: 0.0120  max mem: 1506
Test:  [ 60/348]  eta: 0:00:42  loss: 0.2506 (0.2493)  acc1: 100.0000 (99.6926)  acc5: 100.0000 (100.0000)  time: 0.1294  data: 0.0119  max mem: 1506
Test:  [ 70/348]  eta: 0:00:40  loss: 0.2541 (0.2502)  acc1: 100.0000 (99.6772)  acc5: 100.0000 (100.0000)  time: 0.1290  data: 0.0115  max mem: 1506
Test:  [ 80/348]  eta: 0:00:38  loss: 0.2536 (0.2498)  acc1: 100.0000 (99.6914)  acc5: 100.0000 (100.0000)  time: 0.1290  data: 0.0114  max mem: 1506
Test:  [ 90/348]  eta: 0:00:36  loss: 0.2457 (0.2496)  acc1: 100.0000 (99.6948)  acc5: 100.0000 (100.0000)  time: 0.1290  data: 0.0114  max mem: 1506
Test:  [100/348]  eta: 0:00:34  loss: 0.2458 (0.2496)  acc1: 100.0000 (99.7043)  acc5: 100.0000 (100.0000)  time: 0.1290  data: 0.0114  max mem: 1506
Test:  [110/348]  eta: 0:00:33  loss: 0.2485 (0.2498)  acc1: 100.0000 (99.6809)  acc5: 100.0000 (100.0000)  time: 0.1291  data: 0.0115  max mem: 1506
Test:  [120/348]  eta: 0:00:31  loss: 0.2488 (0.2499)  acc1: 99.3056 (99.6671)  acc5: 100.0000 (100.0000)  time: 0.1291  data: 0.0115  max mem: 1506
Test:  [130/348]  eta: 0:00:30  loss: 0.2488 (0.2502)  acc1: 99.3056 (99.6607)  acc5: 100.0000 (100.0000)  time: 0.1290  data: 0.0114  max mem: 1506
Test:  [140/348]  eta: 0:00:28  loss: 0.2548 (0.2501)  acc1: 100.0000 (99.6651)  acc5: 100.0000 (100.0000)  time: 0.1290  data: 0.0114  max mem: 1506
Test:  [150/348]  eta: 0:00:27  loss: 0.2558 (0.2504)  acc1: 100.0000 (99.6643)  acc5: 100.0000 (100.0000)  time: 0.1295  data: 0.0116  max mem: 1506
Test:  [160/348]  eta: 0:00:25  loss: 0.2483 (0.2501)  acc1: 100.0000 (99.6722)  acc5: 100.0000 (100.0000)  time: 0.1313  data: 0.0117  max mem: 1506
Test:  [170/348]  eta: 0:00:24  loss: 0.2532 (0.2505)  acc1: 100.0000 (99.6670)  acc5: 100.0000 (100.0000)  time: 0.1319  data: 0.0112  max mem: 1506
Test:  [180/348]  eta: 0:00:22  loss: 0.2532 (0.2506)  acc1: 99.3056 (99.6624)  acc5: 100.0000 (100.0000)  time: 0.1309  data: 0.0115  max mem: 1506
Test:  [190/348]  eta: 0:00:21  loss: 0.2502 (0.2506)  acc1: 100.0000 (99.6691)  acc5: 100.0000 (100.0000)  time: 0.1302  data: 0.0120  max mem: 1506
Test:  [200/348]  eta: 0:00:20  loss: 0.2545 (0.2509)  acc1: 100.0000 (99.6649)  acc5: 100.0000 (100.0000)  time: 0.1300  data: 0.0119  max mem: 1506
Test:  [210/348]  eta: 0:00:18  loss: 0.2544 (0.2508)  acc1: 100.0000 (99.6643)  acc5: 100.0000 (100.0000)  time: 0.1297  data: 0.0118  max mem: 1506
Test:  [220/348]  eta: 0:00:17  loss: 0.2499 (0.2508)  acc1: 100.0000 (99.6638)  acc5: 100.0000 (100.0000)  time: 0.1295  data: 0.0116  max mem: 1506
Test:  [230/348]  eta: 0:00:15  loss: 0.2488 (0.2507)  acc1: 100.0000 (99.6603)  acc5: 100.0000 (100.0000)  time: 0.1298  data: 0.0117  max mem: 1506
Test:  [240/348]  eta: 0:00:14  loss: 0.2450 (0.2504)  acc1: 100.0000 (99.6686)  acc5: 100.0000 (100.0000)  time: 0.1298  data: 0.0117  max mem: 1506
Test:  [250/348]  eta: 0:00:13  loss: 0.2490 (0.2505)  acc1: 100.0000 (99.6652)  acc5: 100.0000 (100.0000)  time: 0.1295  data: 0.0116  max mem: 1506
Test:  [260/348]  eta: 0:00:11  loss: 0.2505 (0.2504)  acc1: 100.0000 (99.6701)  acc5: 100.0000 (100.0000)  time: 0.1293  data: 0.0115  max mem: 1506
Test:  [270/348]  eta: 0:00:10  loss: 0.2448 (0.2502)  acc1: 100.0000 (99.6720)  acc5: 100.0000 (100.0000)  time: 0.1292  data: 0.0113  max mem: 1506
Test:  [280/348]  eta: 0:00:09  loss: 0.2484 (0.2502)  acc1: 100.0000 (99.6713)  acc5: 100.0000 (100.0000)  time: 0.1292  data: 0.0114  max mem: 1506
Test:  [290/348]  eta: 0:00:07  loss: 0.2520 (0.2503)  acc1: 100.0000 (99.6754)  acc5: 100.0000 (100.0000)  time: 0.1292  data: 0.0113  max mem: 1506
Test:  [300/348]  eta: 0:00:06  loss: 0.2499 (0.2505)  acc1: 100.0000 (99.6724)  acc5: 100.0000 (100.0000)  time: 0.1292  data: 0.0113  max mem: 1506
Test:  [310/348]  eta: 0:00:05  loss: 0.2587 (0.2507)  acc1: 100.0000 (99.6695)  acc5: 100.0000 (100.0000)  time: 0.1292  data: 0.0113  max mem: 1506
Test:  [320/348]  eta: 0:00:03  loss: 0.2491 (0.2507)  acc1: 100.0000 (99.6690)  acc5: 100.0000 (100.0000)  time: 0.1295  data: 0.0116  max mem: 1506
Test:  [330/348]  eta: 0:00:02  loss: 0.2463 (0.2510)  acc1: 100.0000 (99.6622)  acc5: 100.0000 (100.0000)  time: 0.1297  data: 0.0117  max mem: 1506
Test:  [340/348]  eta: 0:00:01  loss: 0.2513 (0.2509)  acc1: 100.0000 (99.6640)  acc5: 100.0000 (100.0000)  time: 0.1290  data: 0.0111  max mem: 1506
Test:  [347/348]  eta: 0:00:00  loss: 0.2463 (0.2510)  acc1: 100.0000 (99.6640)  acc5: 100.0000 (100.0000)  time: 0.1388  data: 0.0108  max mem: 1506
Test: Total time: 0:00:46 (0.1334 s / it)
* Acc@1 99.664 Acc@5 100.000 loss 0.251
Accuracy of the network on the 50000 test images: 99.7%
