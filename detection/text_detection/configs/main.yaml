defaults:
  - _self_
  - model: vl4str   # visual decoder and cross modal decoder
  - charset: 94_full
  - dataset: real

# abs_root: /PUT/YOUR/PATH/HERE
abs_root: /root

model:
  _convert_: all
  img_size: [ 32, 128 ]  # [ height, width ]
  max_label_length: 25
  # The ordering in charset_train matters. It determines the token IDs assigned to each character.
  charset_train: ???
  # For charset_test, ordering doesn't matter.
  charset_test: "0123456789abcdefghijklmnopqrstuvwxyz"
  batch_size: 384
  weight_decay: 0.0
  warmup_pct: 0.075  # equivalent to 1.5 epochs of warm up
  code_path: ${abs_root}/dilateformer-main-DwConv-dcn/detection/text_detection

data:
  _target_: data.module.SceneTextDataModule
  root_dir: ${abs_root}/data/str_dataset
  output_url: null
  train_dir: ???
  batch_size: ${model.batch_size}
  img_size: ${model.img_size}
  charset_train: ${model.charset_train}
  charset_test: ${model.charset_test}
  max_label_length: ${model.max_label_length}
  remove_whitespace: true
  normalize_unicode: true
  augment: true
  num_workers: 8
  openai_meanstd: true

trainer:
  _target_: pytorch_lightning.Trainer
  _convert_: all
#  check_val_every_n_epoch: null
  val_check_interval: 0.5
  #max_steps: 169680  # 20 epochs x 8484 steps (for batch size = 384, real data)
  max_epochs: 20
  gradient_clip_val: 20
  gpus: 1
#  accelerator: 'gpu'
#  devices: 2
#  strategy: "ddp"
  precision: 16
  accumulate_grad_batches: 1

ckpt_path: null
pretrained: null
swa: false

hydra:
  output_subdir: config
  run:
    # use absolute dir here for full control
    dir: ${abs_root}/output/${model.name}_${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir: ${abs_root}/output/${model.name}_${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ${hydra.job.override_dirname}
